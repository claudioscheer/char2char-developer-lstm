import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd


random.seed(0)


def generate_sequence(length=25):
    return [random.randint(0, 99) for _ in range(length)]


def one_hot_encode(sequence, n_unique=100):
    encoding = list()
    for x in sequence:
        vector = [0 for _ in range(n_unique)]
        vector[x] = 1
        encoding.append(vector)
    return np.array(encoding)


def one_hot_decode(sequence):
    return [np.argmax(x) for x in sequence]


def to_dataset(encoded_sequence, size_x=5, size_y=3):
    df = pd.DataFrame(encoded_sequence)
    df = pd.concat([df.shift(size_x - i - 1) for i in range(size_x)], axis=1)
    # Drop rows with NaN (missing values).
    df.dropna(inplace=True)
    # Return DataFrame as an array.
    values = df.values
    # Size of the arrays that have the encoded values.
    width = encoded_sequence.shape[1]
    x = values.reshape(len(values), size_x, width)
    y = values[:, 0 : (size_y * width)].reshape(len(values), size_y, width)
    return x, y


def get_data(length_data, size_x, size_y):
    sequence = generate_sequence(length_data)
    encoded_sequence = one_hot_encode(sequence)
    x, y = to_dataset(encoded_sequence, size_x, size_y)
    return x, y


model = nn.LSTM(100, 256, 2)
model.cuda()
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

x, y = get_data(100, 5, 5)
print(x[:10].shape)
print(y.shape)


# epochs = 1000
# for _ in range(epochs):
#     h = model.init_hidden(n_seqs)
